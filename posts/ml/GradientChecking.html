<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>nbloomf.blog - Gradient Checking</title>
<link rel="stylesheet" type="text/css" href="../../css/default.css" />
<link rel="icon" href="../../raw/gfx/icon/favicon-32.png" />
<link rel="apple-touch-icon-precomposed" sizes="57x57" href="../../raw/gfx/icon/favicon-57.png" />
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="../../raw/gfx/icon/favicon-114.png" />
<link rel="apple-touch-icon-precomposed" sizes="152x152" href="../../raw/gfx/icon/favicon-152.png" />
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>

<body>
<div id="header">
  <div id="logo">
    <a href="../../index.html">nbloomf</a>
  </div>
  <div id="navigation">
    <a href="../../index.html">Home</a>
    <a href="../../pages/about.html">About</a>
    <a href="../../pages/projects.html">Projects</a>
    <a href="../../archive.html">Blog</a>
  </div>
</div>

<div id="content">
<h1>Gradient Checking</h1>
<!-- BEGIN BODY -->


<div class="info">
Posted on 2017-10-18 by nbloomf
</div>


<div class="info tags">Tags: <a href="../../tag/ml.html">ml</a>, <a href="../../tag/literate-haskell.html">literate-haskell</a></div>


<p class="post-info">This post is part of a series of notes on <a href="../../pages/ml.html">machine learning</a>.</p>

<p class="post-info">This post is literate Haskell; you can load <a href="https://raw.githubusercontent.com/nbloomf/nbloomf.md/master/posts/ml/GradientChecking.lhs">the source</a> into GHCi and play along.</p>

<hr />

<p>Boilerplate.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> <span class="kw">module</span> <span class="dt">GradientChecking</span> <span class="kw">where</span>
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> <span class="kw">import </span><span class="dt">Control.Applicative</span>
<span class="ot">&gt;</span> <span class="kw">import </span><span class="dt">Test.QuickCheck</span>
<span class="ot">&gt;</span> <span class="kw">import </span><span class="dt">Test.QuickCheck.Test</span>
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> <span class="kw">import </span><span class="dt">Indices</span>
<span class="ot">&gt;</span> <span class="kw">import </span><span class="dt">IndexIsos</span>
<span class="ot">&gt;</span> <span class="kw">import </span><span class="dt">Tensors</span>
<span class="ot">&gt;</span> <span class="kw">import </span><span class="dt">TensorFunctions</span>
<span class="ot">&gt;</span> <span class="kw">import </span><span class="dt">Gradients</span></code></pre></div>
<p>Computing gradients of multivariate <span class="math inline">\(f\)</span> is intricate, so it’d be nice to have a simple (if slow) way to check them for reasonableness. In this post we’ll see two methods for doing exactly that.</p>
<h2 id="numerical-approximation">Numerical Approximation</h2>
<p>In the single variable case the gradient at a point <span class="math inline">\(v\)</span> is the slope of the line tangent to <span class="math inline">\(f\)</span> at <span class="math inline">\(v\)</span>, which can be approximated using the slope of the secant line passing through the points on <span class="math inline">\(f\)</span> at <span class="math inline">\(v \pm \varepsilon\)</span> for small epsilon, sometimes called the <em>difference quotient</em>. In math notation, the difference quotient is <span class="math display">\[\frac{f(v + \varepsilon) - f(v - \varepsilon)}{2 \varepsilon},\]</span> and in code we say:</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt; diffquo ::</span> (<span class="dt">Num</span> r, <span class="dt">Fractional</span> r) <span class="ot">=&gt;</span> (r <span class="ot">-&gt;</span> r) <span class="ot">-&gt;</span> r <span class="ot">-&gt;</span> r <span class="ot">-&gt;</span> r
<span class="ot">&gt;</span> diffquo f x eps <span class="fu">=</span> ((f <span class="fu">$</span> x <span class="fu">+</span> eps) <span class="fu">-</span> (f <span class="fu">$</span> x <span class="fu">-</span> eps)) <span class="fu">/</span> (<span class="dv">2</span><span class="fu">*</span>eps)</code></pre></div>
<p>Of course the gradient is a bunch of pointwise univariate derivatives, so we can approximate the gradient pointwise.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> approxGrad
<span class="ot">&gt;   ::</span> (<span class="dt">Num</span> r, <span class="dt">Fractional</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r          <span class="co">-- pointwise epsilon</span>
<span class="ot">&gt;</span>   <span class="ot">-&gt;</span> <span class="dt">Function</span> r <span class="co">-- function to approximate</span>
<span class="ot">&gt;</span>   <span class="ot">-&gt;</span> <span class="dt">Function</span> r <span class="co">-- gradient</span>
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> approxGrad eps f <span class="fu">=</span> <span class="dt">F</span>
<span class="ot">&gt;</span>   { dom <span class="fu">=</span> dom f
<span class="ot">&gt;</span>   , cod <span class="fu">=</span> (cod f) <span class="fu">:*</span> (dom f)
<span class="ot">&gt;</span>   , fun <span class="fu">=</span> \v <span class="ot">-&gt;</span>
<span class="ot">&gt;</span>       <span class="kw">let</span>
<span class="ot">&gt;</span>         g i j x  <span class="fu">=</span> (f <span class="fu">$@</span> (inj i v x))<span class="ot">`at`</span>j
<span class="ot">&gt;</span>       <span class="kw">in</span>
<span class="ot">&gt;</span>         tensor ((cod f) <span class="fu">:*</span> (dom f)) <span class="fu">$</span>
<span class="ot">&gt;</span>           \(j <span class="fu">:&amp;</span> i) <span class="ot">-&gt;</span> diffquo (g i j) (v<span class="ot">`at`</span>i) eps
<span class="ot">&gt;</span>   }
<span class="ot">&gt;</span>   <span class="kw">where</span>
<span class="ot">&gt;     inj ::</span> (<span class="dt">Num</span> r) <span class="ot">=&gt;</span> <span class="dt">Index</span> <span class="ot">-&gt;</span> <span class="dt">Tensor</span> r <span class="ot">-&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Tensor</span> r
<span class="ot">&gt;</span>     inj k a<span class="fu">@</span>(<span class="dt">T</span> u _) x <span class="fu">=</span> tensor u <span class="fu">$</span>
<span class="ot">&gt;</span>       \i <span class="ot">-&gt;</span> <span class="kw">if</span> i <span class="fu">==</span> k <span class="kw">then</span> x <span class="kw">else</span> a<span class="ot">`at`</span>i</code></pre></div>
<p>So <code>approxGrad</code> computes the numerical gradient of a function using some tolerance <code>eps</code>. Testing it out by hand:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="fu">$&gt;</span> <span class="kw">let</span> f <span class="fu">=</span> pointwiseF <span class="dv">3</span> sin
<span class="fu">$&gt;</span> <span class="kw">let</span> g <span class="fu">=</span> approxGrad <span class="fl">0.00001</span> f
<span class="fu">$&gt;</span> g <span class="fu">$@</span> (vec [<span class="dv">0</span>,pi<span class="fu">/</span><span class="dv">2</span>,pi])
 <span class="fl">0.9999999999833332</span>                 <span class="fl">0.0</span>                 <span class="fl">0.0</span>
                <span class="fl">0.0</span>                 <span class="fl">0.0</span>                 <span class="fl">0.0</span>
                <span class="fl">0.0</span>                 <span class="fl">0.0</span> <span class="fu">-</span><span class="fl">0.9999999999898844</span></code></pre></div>
<p>Which is not terrible.</p>
<p>If we have some other function that claims to compute the gradient, a decent reasonableness check is to see whether the numerical gradient at <span class="math inline">\(v\)</span> and the “exact” gradient at <span class="math inline">\(v\)</span> are equal, or, more likely, very close to each other. “Very close” for tensors can be measured in a few ways, each appropriate in different cases. We’ll use a type to toggle among them.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> <span class="kw">data</span> <span class="dt">Metric</span>
<span class="ot">&gt;</span>   <span class="fu">=</span> <span class="dt">MaxAbsDiff</span> <span class="co">-- max entrywise absolute difference</span>
<span class="ot">&gt;</span>   <span class="fu">|</span> <span class="dt">MaxRelDiff</span> <span class="co">-- max entrywise relative difference</span>
<span class="ot">&gt;</span>   <span class="kw">deriving</span> (<span class="dt">Eq</span>, <span class="dt">Show</span>)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> distanceBy
<span class="ot">&gt;   ::</span> (<span class="dt">Num</span> r, <span class="dt">Ord</span> r, <span class="dt">Fractional</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> <span class="dt">Metric</span> <span class="ot">-&gt;</span> <span class="dt">Tensor</span> r <span class="ot">-&gt;</span> <span class="dt">Tensor</span> r <span class="ot">-&gt;</span> r
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> distanceBy m a b <span class="fu">=</span> <span class="kw">case</span> m <span class="kw">of</span>
<span class="ot">&gt;</span>   <span class="dt">MaxAbsDiff</span> <span class="ot">-&gt;</span> maximum <span class="fu">$</span> tzipWith f a b
<span class="ot">&gt;</span>     <span class="kw">where</span> f x y <span class="fu">=</span> abs (x <span class="fu">-</span> y)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   <span class="dt">MaxRelDiff</span> <span class="ot">-&gt;</span> maximum <span class="fu">$</span> tzipWith f a b
<span class="ot">&gt;</span>     <span class="kw">where</span> f x y <span class="fu">=</span> abs <span class="fu">$</span> (x <span class="fu">-</span> y) <span class="fu">/</span> (max x y)</code></pre></div>
<p>And a helper test for function equality:</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> _test_functions_equal
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Ord</span> r, <span class="dt">Num</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> <span class="dt">Metric</span> <span class="ot">-&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Function</span> r <span class="ot">-&gt;</span> <span class="dt">Function</span> r <span class="ot">-&gt;</span> <span class="dt">Property</span>
<span class="ot">&gt;</span> _test_functions_equal metric eps f g <span class="fu">=</span>
<span class="ot">&gt;</span>   forAll (arbTensorOf eps (dom f)) <span class="fu">$</span>
<span class="ot">&gt;</span>     \v <span class="ot">-&gt;</span> distanceBy metric (f <span class="fu">$@</span> v) (g <span class="fu">$@</span> v) <span class="fu">&lt;</span> eps</code></pre></div>
<p>For example, the gradient of scalar multiplication should be a scalar multiple of the “identity” tensor; the following test verifies this.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> _test_numerical_scalar_gradient
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Num</span> r, <span class="dt">Ord</span> r, <span class="dt">Fractional</span> r, <span class="dt">Floating</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> _test_numerical_scalar_gradient _ <span class="fu">=</span>
<span class="ot">&gt;</span>   testName <span class="st">&quot;numerical scalar gradient&quot;</span> <span class="fu">$</span>
<span class="ot">&gt;</span>   \u k <span class="ot">-&gt;</span> u <span class="fu">~/=</span> <span class="dv">0</span> <span class="fu">==&gt;</span> 
<span class="ot">&gt;</span>     _test_functions_equal <span class="dt">MaxAbsDiff</span> (<span class="dv">10</span><span class="fu">**</span>(<span class="fu">-</span><span class="dv">6</span>))
<span class="ot">&gt;</span>       (approxGrad (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">6</span>)) <span class="fu">$</span> scalarF u k)
<span class="ot">&gt;</span>       (constF u (k <span class="fu">.@</span> (idMat u)))</code></pre></div>
<p>More generally, the gradient of a “linear” tensor function is a constant.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> _test_numerical_linear_gradient
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Ord</span> r, <span class="dt">Num</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> _test_numerical_linear_gradient r <span class="fu">=</span>
<span class="ot">&gt;</span>   testName <span class="st">&quot;numerical linear gradient&quot;</span> <span class="fu">$</span>
<span class="ot">&gt;</span>   \u v <span class="ot">-&gt;</span> (u <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">&amp;&amp;</span> (v <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">==&gt;</span>
<span class="ot">&gt;</span>     forAll (arbTensorOf r (v <span class="fu">:*</span> u)) <span class="fu">$</span>
<span class="ot">&gt;</span>       \m <span class="ot">-&gt;</span>
<span class="ot">&gt;</span>         _test_functions_equal <span class="dt">MaxAbsDiff</span> (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">5</span>))
<span class="ot">&gt;</span>           (approxGrad (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">5</span>)) <span class="fu">$</span> linearF m)
<span class="ot">&gt;</span>           (constF u m)</code></pre></div>
<p>Yet more generally still, the gradient of an “affine” tensor function is a constant.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> _test_numerical_affine_gradient
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Ord</span> r, <span class="dt">Num</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> _test_numerical_affine_gradient r <span class="fu">=</span>
<span class="ot">&gt;</span>   testName <span class="st">&quot;numerical affine gradient&quot;</span> <span class="fu">$</span>
<span class="ot">&gt;</span>   \u v <span class="ot">-&gt;</span> (u <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">&amp;&amp;</span> (v <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">==&gt;</span>
<span class="ot">&gt;</span>     forAll2 (arbTensorOf r (v <span class="fu">:*</span> u)) (arbTensorOf r v) <span class="fu">$</span>
<span class="ot">&gt;</span>       \m b <span class="ot">-&gt;</span>
<span class="ot">&gt;</span>         _test_functions_equal <span class="dt">MaxAbsDiff</span> (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">5</span>))
<span class="ot">&gt;</span>           (approxGrad (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">5</span>)) <span class="fu">$</span> affineF m b)
<span class="ot">&gt;</span>           (constF u m)</code></pre></div>
<p>Pointwise functions (parameterized on the metric):</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> _test_numerical_pointwise_gradient_by
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Num</span> r, <span class="dt">Ord</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> <span class="dt">Metric</span> <span class="ot">-&gt;</span> <span class="dt">String</span> <span class="ot">-&gt;</span> r <span class="ot">-&gt;</span> (r <span class="ot">-&gt;</span> r) <span class="ot">-&gt;</span> (r <span class="ot">-&gt;</span> r)
<span class="ot">&gt;</span>   <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> _test_numerical_pointwise_gradient_by metric str r p q <span class="fu">=</span>
<span class="ot">&gt;</span>   testName (<span class="st">&quot;numerical pointwise gradient (&quot;</span> <span class="fu">++</span> str <span class="fu">++</span> <span class="st">&quot;)&quot;</span>) <span class="fu">$</span>
<span class="ot">&gt;</span>   \u <span class="ot">-&gt;</span> u <span class="fu">~/=</span> <span class="dv">0</span> <span class="fu">==&gt;</span> 
<span class="ot">&gt;</span>     _test_functions_equal metric (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">4</span>))
<span class="ot">&gt;</span>       (approxGrad (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">4</span>)) <span class="fu">$</span> pointwiseF u p)
<span class="ot">&gt;</span>       (diagF u <span class="fu">$.</span> (pointwiseF u q))</code></pre></div>
<p>The gradient of direct summing is constant.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> _test_numerical_direct_sum_left_gradient
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Num</span> r, <span class="dt">Ord</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> _test_numerical_direct_sum_left_gradient r <span class="fu">=</span>
<span class="ot">&gt;</span>   testName <span class="st">&quot;direct sum on left numerical gradient&quot;</span> <span class="fu">$</span>
<span class="ot">&gt;</span>   \u v <span class="ot">-&gt;</span> (u <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">&amp;&amp;</span> (v <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">==&gt;</span>
<span class="ot">&gt;</span>     forAll (arbTensorOf r v) <span class="fu">$</span>
<span class="ot">&gt;</span>       \m <span class="ot">-&gt;</span>
<span class="ot">&gt;</span>         _test_functions_equal <span class="dt">MaxAbsDiff</span> (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">2</span>))
<span class="ot">&gt;</span>           (approxGrad (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">6</span>)) <span class="fu">$</span> dSumL u m)
<span class="ot">&gt;</span>           (constF u <span class="fu">$</span> (zeros <span class="fu">$</span> v <span class="fu">:*</span> u) <span class="fu">~-~</span> (idMat u))
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> _test_numerical_direct_sum_right_gradient
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Num</span> r, <span class="dt">Ord</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> _test_numerical_direct_sum_right_gradient r <span class="fu">=</span>
<span class="ot">&gt;</span>   testName <span class="st">&quot;direct sum on right numerical gradient&quot;</span> <span class="fu">$</span>
<span class="ot">&gt;</span>   \u v <span class="ot">-&gt;</span> (u <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">&amp;&amp;</span> (v <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">==&gt;</span>
<span class="ot">&gt;</span>     forAll (arbTensorOf r v) <span class="fu">$</span>
<span class="ot">&gt;</span>       \m <span class="ot">-&gt;</span>
<span class="ot">&gt;</span>         _test_functions_equal <span class="dt">MaxAbsDiff</span> (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">2</span>))
<span class="ot">&gt;</span>           (approxGrad (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">6</span>)) <span class="fu">$</span> dSumR u m)
<span class="ot">&gt;</span>           (constF u <span class="fu">$</span> (idMat u) <span class="fu">~-~</span> (zeros <span class="fu">$</span> v <span class="fu">:*</span> u))</code></pre></div>
<h2 id="dual-numbers">Dual Numbers</h2>
<p>Another neat trick for computing the gradient of a function is automatic differentiation using <em>dual numbers</em>. For the single variable case, this method works by embedding <span class="math inline">\(\mathbb{R}\)</span> in the ring <span class="math inline">\(\mathbb{R}[x]/(x^2)\)</span>. Given <span class="math inline">\(a + bx\)</span> in this ring, <span class="math inline">\(a\)</span> is called the <em>real part</em> and <span class="math inline">\(b\)</span> the <em>infinitesimal part</em>. Carrying out arithmetic as usual makes <span class="math inline">\(b\)</span> act like the derivative of <span class="math inline">\(a\)</span>. I won’t go into the details, mostly because they’re a little bit magic to me still, but this can be extended to the multivariate case, as we’ll do.</p>
<p>Remember that the gradient of a tensor function is again a tensor. So rather than keeping track of a number and its derivative, we’ll keep track of a number and its gradient tensor.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> <span class="kw">data</span> <span class="dt">Dual</span> r
<span class="ot">&gt;</span>   <span class="fu">=</span> <span class="dt">D</span> r (<span class="dt">Tensor</span> r)
<span class="ot">&gt;</span>   <span class="kw">deriving</span> (<span class="dt">Show</span>)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> <span class="co">-- we'll think of the empty gradient as</span>
<span class="ot">&gt;</span> <span class="co">-- representing a constant of any size.</span>
<span class="ot">&gt; toDual ::</span> r <span class="ot">-&gt;</span> <span class="dt">Dual</span> r
<span class="ot">&gt;</span> toDual x <span class="fu">=</span> <span class="dt">D</span> x empty
<span class="ot">&gt;</span> 
<span class="ot">&gt; unDual ::</span> <span class="dt">Dual</span> r <span class="ot">-&gt;</span> r
<span class="ot">&gt;</span> unDual (<span class="dt">D</span> x _) <span class="fu">=</span> x</code></pre></div>
<p>Thinking of a tensor as a “variable”, we dualize by embedding each coordinate with its partial derivative.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt; dualize ::</span> (<span class="dt">Num</span> r) <span class="ot">=&gt;</span> <span class="dt">Tensor</span> r <span class="ot">-&gt;</span> <span class="dt">Tensor</span> (<span class="dt">Dual</span> r)
<span class="ot">&gt;</span> dualize a<span class="fu">@</span>(<span class="dt">T</span> u _) <span class="fu">=</span> tensor u (\i <span class="ot">-&gt;</span> var u i (a<span class="ot">`at`</span>i))
<span class="ot">&gt;</span>   <span class="kw">where</span>
<span class="ot">&gt;     var ::</span> (<span class="dt">Num</span> r) <span class="ot">=&gt;</span> <span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Index</span> <span class="ot">-&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Dual</span> r
<span class="ot">&gt;</span>     var u k r <span class="fu">=</span> <span class="dt">D</span> r (tensor u (\i <span class="ot">-&gt;</span> <span class="kw">if</span> i <span class="fu">==</span> k <span class="kw">then</span> <span class="dv">1</span> <span class="kw">else</span> <span class="dv">0</span>))</code></pre></div>
<p>Now we define arithmetic on dual numbers in a way that essentially encodes the chain rule at each step. With the magic of type classes, now anytime we define a function with signature <code>(Num r) =&gt; Function r</code>, it can be evaluated over the dual numbers with no changes.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> <span class="kw">instance</span> (<span class="dt">Eq</span> r) <span class="ot">=&gt;</span> <span class="dt">Eq</span> (<span class="dt">Dual</span> r) <span class="kw">where</span>
<span class="ot">&gt;</span>   (<span class="dt">D</span> x _) <span class="fu">==</span> (<span class="dt">D</span> y _) <span class="fu">=</span> x <span class="fu">==</span> y
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> <span class="kw">instance</span> (<span class="dt">Num</span> r) <span class="ot">=&gt;</span> <span class="dt">Num</span> (<span class="dt">Dual</span> r) <span class="kw">where</span>
<span class="ot">&gt;</span>   fromInteger x <span class="fu">=</span> toDual (fromInteger x)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   (<span class="dt">D</span> x dx) <span class="fu">+</span> (<span class="dt">D</span> y dy)
<span class="ot">&gt;</span>     <span class="fu">|</span> (size dx) <span class="fu">~=</span> <span class="dv">0</span> <span class="fu">=</span> <span class="dt">D</span> (x <span class="fu">+</span> y) dy
<span class="ot">&gt;</span>     <span class="fu">|</span> (size dy) <span class="fu">~=</span> <span class="dv">0</span> <span class="fu">=</span> <span class="dt">D</span> (x <span class="fu">+</span> y) dx
<span class="ot">&gt;</span>     <span class="fu">|</span> otherwise      <span class="fu">=</span> <span class="dt">D</span> (x <span class="fu">+</span> y) (dx <span class="fu">.+</span> dy)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   (<span class="dt">D</span> x dx) <span class="fu">*</span> (<span class="dt">D</span> y dy)
<span class="ot">&gt;</span>     <span class="fu">|</span> (size dx) <span class="fu">~=</span> <span class="dv">0</span> <span class="fu">=</span> x <span class="fu">.@</span> (<span class="dt">D</span> y dy)
<span class="ot">&gt;</span>     <span class="fu">|</span> (size dy) <span class="fu">~=</span> <span class="dv">0</span> <span class="fu">=</span> y <span class="fu">.@</span> (<span class="dt">D</span> x dx)
<span class="ot">&gt;</span>     <span class="fu">|</span> otherwise      <span class="fu">=</span> <span class="dt">D</span> (x <span class="fu">*</span> y) ((y <span class="fu">.@</span> dx) <span class="fu">.+</span> (x <span class="fu">.@</span> dy))
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   negate (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (negate x) (fmap negate dx)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   abs (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (abs x) ((signum x) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   signum (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (signum x) (<span class="dv">0</span> <span class="fu">.@</span> dx)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> <span class="kw">instance</span> <span class="dt">Vector</span> <span class="dt">Dual</span> <span class="kw">where</span>
<span class="ot">&gt;</span>   r <span class="fu">.@</span> (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (r<span class="fu">*</span>x) (fmap (r<span class="fu">*</span>) dx)
<span class="ot">&gt;</span>   (<span class="fu">.+</span>) <span class="fu">=</span> (<span class="fu">+</span>)
<span class="ot">&gt;</span>   neg <span class="fu">=</span> negate
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> <span class="kw">instance</span> (<span class="dt">Eq</span> r, <span class="dt">Num</span> r, <span class="dt">Ord</span> r) <span class="ot">=&gt;</span> <span class="dt">Ord</span> (<span class="dt">Dual</span> r) <span class="kw">where</span>
<span class="ot">&gt;</span>   (<span class="dt">D</span> x _) <span class="fu">&lt;=</span> (<span class="dt">D</span> y _) <span class="fu">=</span> x <span class="fu">&lt;=</span> y
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   max (<span class="dt">D</span> x dx) (<span class="dt">D</span> y dy) <span class="fu">=</span> <span class="kw">if</span> x <span class="fu">&gt;=</span> y
<span class="ot">&gt;</span>     <span class="kw">then</span> <span class="dt">D</span> x dx <span class="kw">else</span> <span class="dt">D</span> y dy
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   min (<span class="dt">D</span> x dx) (<span class="dt">D</span> y dy) <span class="fu">=</span> <span class="kw">if</span> x <span class="fu">&lt;=</span> y
<span class="ot">&gt;</span>     <span class="kw">then</span> <span class="dt">D</span> x dx <span class="kw">else</span> <span class="dt">D</span> y dy
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> <span class="kw">instance</span> (<span class="dt">Eq</span> r, <span class="dt">Num</span> r, <span class="dt">Fractional</span> r) <span class="ot">=&gt;</span> <span class="dt">Fractional</span> (<span class="dt">Dual</span> r) <span class="kw">where</span>
<span class="ot">&gt;</span>   fromRational x <span class="fu">=</span> toDual (fromRational x)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   recip (<span class="dt">D</span> x dx<span class="fu">@</span>(<span class="dt">T</span> u _)) <span class="fu">=</span> <span class="dt">D</span> (<span class="dv">1</span><span class="fu">/</span>x) ((<span class="fu">-</span><span class="dv">1</span><span class="fu">/</span>x<span class="fu">^</span><span class="dv">2</span>) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> <span class="kw">instance</span> (<span class="dt">Eq</span> r, <span class="dt">Num</span> r, <span class="dt">Floating</span> r) <span class="ot">=&gt;</span> <span class="dt">Floating</span> (<span class="dt">Dual</span> r) <span class="kw">where</span>
<span class="ot">&gt;</span>   pi             <span class="fu">=</span> <span class="dt">D</span> pi        empty
<span class="ot">&gt;</span>   sqrt  (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (sqrt x)  ((<span class="dv">1</span> <span class="fu">/</span> (<span class="dv">2</span><span class="fu">*</span>(sqrt x))) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   exp   (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (exp x)   ((exp x) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   log   (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (log x)   ((<span class="dv">1</span> <span class="fu">/</span> x) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   sin   (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (sin x)   ((cos x) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   cos   (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (cos x)   ((<span class="fu">-</span>(sin x)) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   tan   (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (tan x)   ((<span class="dv">1</span><span class="fu">/</span>(cos x)<span class="fu">**</span><span class="dv">2</span>) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   asin  (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (asin x)  ((<span class="dv">1</span><span class="fu">/</span>(sqrt(<span class="dv">1</span><span class="fu">-</span>x<span class="fu">**</span><span class="dv">2</span>))) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   acos  (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (acos x)  ((<span class="fu">-</span><span class="dv">1</span><span class="fu">/</span>(sqrt(<span class="dv">1</span><span class="fu">-</span>x<span class="fu">**</span><span class="dv">2</span>))) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   atan  (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (atan x)  ((<span class="dv">1</span><span class="fu">/</span>(<span class="dv">1</span><span class="fu">+</span>x<span class="fu">**</span><span class="dv">2</span>)) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   sinh  (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (sinh x)  ((cosh x) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   cosh  (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (cosh x)  ((sinh x) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   tanh  (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (tanh x)  ((<span class="dv">1</span><span class="fu">-</span>(tanh x)<span class="fu">**</span><span class="dv">2</span>) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   asinh (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (asinh x) ((<span class="dv">1</span><span class="fu">/</span>(sqrt(x<span class="fu">**</span><span class="dv">2</span><span class="fu">+</span><span class="dv">1</span>))) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   acosh (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (acosh x) ((<span class="dv">1</span><span class="fu">/</span>(sqrt(x<span class="fu">**</span><span class="dv">2</span><span class="fu">-</span><span class="dv">1</span>))) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span>   atanh (<span class="dt">D</span> x dx) <span class="fu">=</span> <span class="dt">D</span> (atanh x) ((<span class="dv">1</span><span class="fu">/</span>(<span class="dv">1</span><span class="fu">-</span>x<span class="fu">**</span><span class="dv">2</span>)) <span class="fu">.@</span> dx)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   (<span class="dt">D</span> x dx) <span class="fu">**</span> (<span class="dt">D</span> y dy)
<span class="ot">&gt;</span>     <span class="fu">|</span> dx <span class="fu">==</span> empty <span class="fu">=</span> <span class="dt">D</span> (x<span class="fu">**</span>y) ((x<span class="fu">**</span>y) <span class="fu">.@</span> ((log x) <span class="fu">.@</span> dy))
<span class="ot">&gt;</span>     <span class="fu">|</span> dy <span class="fu">==</span> empty <span class="fu">=</span> <span class="dt">D</span> (x<span class="fu">**</span>y) ((x<span class="fu">**</span>y) <span class="fu">.@</span> ((y<span class="fu">/</span>x) <span class="fu">.@</span> dx))
<span class="ot">&gt;</span>     <span class="fu">|</span> otherwise   <span class="fu">=</span> <span class="dt">D</span> (x<span class="fu">**</span>y)
<span class="ot">&gt;</span>         ((x<span class="fu">**</span>y) <span class="fu">.@</span> (((log x) <span class="fu">.@</span> dy) <span class="fu">.+</span> ((y<span class="fu">/</span>x) <span class="fu">.@</span> dx)))
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   logBase (<span class="dt">D</span> x _) (<span class="dt">D</span> y dy) <span class="fu">=</span> <span class="dt">D</span> (logBase x y)
<span class="ot">&gt;</span>     ((<span class="dv">1</span><span class="fu">/</span>(log x)<span class="fu">**</span><span class="dv">2</span>) <span class="fu">.@</span> ((((log y)<span class="fu">/</span>x) <span class="fu">.@</span> dy) <span class="fu">.-</span> (((log x)<span class="fu">/</span>y) <span class="fu">.@</span> dy)))
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> <span class="kw">instance</span> (<span class="dt">Real</span> r) <span class="ot">=&gt;</span> <span class="dt">Real</span> (<span class="dt">Dual</span> r) <span class="kw">where</span>
<span class="ot">&gt;</span>   toRational (<span class="dt">D</span> x _) <span class="fu">=</span> toRational x</code></pre></div>
<p>Now we can automatically take a tensor function on (tensors of) dual numbers, and find its gradient as a function on (tensors of) ordinary numbers.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt; dualGrad ::</span> (<span class="dt">Num</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> <span class="dt">Function</span> (<span class="dt">Dual</span> r) <span class="ot">-&gt;</span> <span class="dt">Function</span> r
<span class="ot">&gt;</span> dualGrad f <span class="fu">=</span> <span class="dt">F</span>
<span class="ot">&gt;</span>   { dom <span class="fu">=</span> dom f
<span class="ot">&gt;</span>   , cod <span class="fu">=</span> (cod f) <span class="fu">:*</span> (dom f)
<span class="ot">&gt;</span>   , fun <span class="fu">=</span> \v <span class="ot">-&gt;</span> tensor ((cod f) <span class="fu">:*</span> (dom f)) <span class="fu">$</span>
<span class="ot">&gt;</span>       \(i <span class="fu">:&amp;</span> j) <span class="ot">-&gt;</span>
<span class="ot">&gt;</span>         <span class="kw">let</span> (<span class="dt">D</span> _ dx) <span class="fu">=</span> (f <span class="fu">$@</span> (dualize v)) <span class="ot">`at`</span> i
<span class="ot">&gt;</span>         <span class="kw">in</span> dx <span class="ot">`at`</span> j
<span class="ot">&gt;</span>   }</code></pre></div>
<p>To be clear: we can now evaluate the gradient of an arbitrary tensor function. As a quick hand test:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="fu">$&gt;</span> <span class="kw">let</span> f <span class="fu">=</span> pointwiseF <span class="dv">1</span> sin <span class="co">-- f(x) = sin(x)</span>
<span class="fu">$&gt;</span> <span class="kw">let</span> g <span class="fu">=</span> dualGrad f <span class="co">-- magic</span>
<span class="fu">$&gt;</span> g <span class="fu">$@</span> (cell pi)
<span class="fu">-</span><span class="fl">1.0</span>
<span class="fu">$&gt;</span> <span class="kw">let</span> f <span class="fu">=</span> pointwiseF <span class="dv">3</span> (<span class="fu">^</span><span class="dv">2</span>) <span class="co">-- f(x,y,z) = (x^2,y^2,z^2)</span>
<span class="fu">$&gt;</span> <span class="kw">let</span> g <span class="fu">=</span> dualGrad f <span class="co">-- magic</span>
<span class="fu">$&gt;</span> g <span class="fu">$@</span> (vec [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])
<span class="dv">2</span> <span class="dv">0</span> <span class="dv">0</span>
<span class="dv">0</span> <span class="dv">4</span> <span class="dv">0</span>
<span class="dv">0</span> <span class="dv">0</span> <span class="dv">6</span></code></pre></div>
<p>I will probably never get used to that. Anyway, like we did with numeric differentiation, we can use automatic differentiation to test a claimed “exact” gradient. By the way – AD gives us the exact gradient, so why don’t we just use it? Unfortunately AD ends up doing a lot of work to keep track of those intermediate derivatives, so a dedicated gradient function (if we can find it) can be more efficient. But we can still test our handwritten gradients against the AD gradient or numeric gradient on small functions to gain confidence that we’re doing it right.</p>
<p>We need an <code>Arbitrary</code> instance for dual numbers:</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> <span class="kw">instance</span> (<span class="dt">Arbitrary</span> r) <span class="ot">=&gt;</span> <span class="dt">Arbitrary</span> (<span class="dt">Dual</span> r) <span class="kw">where</span>
<span class="ot">&gt;</span>   arbitrary <span class="fu">=</span> <span class="kw">do</span>
<span class="ot">&gt;</span>     x <span class="ot">&lt;-</span> arbitrary
<span class="ot">&gt;</span>     return <span class="fu">$</span> <span class="dt">D</span> x empty</code></pre></div>
<p>And we can compare an alleged gradient function against the automatically computed gradient.</p>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> _test_dual_scalar_gradient
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Num</span> r, <span class="dt">Ord</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Dual</span> r <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> _test_dual_scalar_gradient _ <span class="fu">=</span>
<span class="ot">&gt;</span>   testName <span class="st">&quot;dual scalar gradient&quot;</span> <span class="fu">$</span>
<span class="ot">&gt;</span>   \u k <span class="ot">-&gt;</span> u <span class="fu">~/=</span> <span class="dv">0</span> <span class="fu">==&gt;</span> 
<span class="ot">&gt;</span>     _test_functions_equal <span class="dt">MaxAbsDiff</span> (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">6</span>))
<span class="ot">&gt;</span>       (dualGrad <span class="fu">$</span> scalarF u k)
<span class="ot">&gt;</span>       (constF u ((unDual k) <span class="fu">.@</span> (idMat u)))
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> _test_dual_linear_gradient
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Ord</span> r, <span class="dt">Num</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> _test_dual_linear_gradient r <span class="fu">=</span>
<span class="ot">&gt;</span>   testName <span class="st">&quot;dual linear gradient&quot;</span> <span class="fu">$</span>
<span class="ot">&gt;</span>   \u v <span class="ot">-&gt;</span> (u <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">&amp;&amp;</span> (v <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">==&gt;</span>
<span class="ot">&gt;</span>     forAll (arbTensorOf (toDual r) (v <span class="fu">:*</span> u)) <span class="fu">$</span>
<span class="ot">&gt;</span>       \m <span class="ot">-&gt;</span>
<span class="ot">&gt;</span>         _test_functions_equal <span class="dt">MaxAbsDiff</span> (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">5</span>))
<span class="ot">&gt;</span>           (dualGrad <span class="fu">$</span> linearF m)
<span class="ot">&gt;</span>           (constF u (fmap unDual m))
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> _test_dual_affine_gradient
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Ord</span> r, <span class="dt">Num</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> _test_dual_affine_gradient r <span class="fu">=</span>
<span class="ot">&gt;</span>   testName <span class="st">&quot;dual affine gradient&quot;</span> <span class="fu">$</span>
<span class="ot">&gt;</span>   \u v <span class="ot">-&gt;</span> (u <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">&amp;&amp;</span> (v <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">==&gt;</span>
<span class="ot">&gt;</span>     forAll2
<span class="ot">&gt;</span>       (arbTensorOf (toDual r) (v <span class="fu">:*</span> u))
<span class="ot">&gt;</span>       (arbTensorOf (toDual r) v) <span class="fu">$</span>
<span class="ot">&gt;</span>       \m b <span class="ot">-&gt;</span>
<span class="ot">&gt;</span>         _test_functions_equal <span class="dt">MaxAbsDiff</span> (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">5</span>))
<span class="ot">&gt;</span>           (dualGrad <span class="fu">$</span> affineF m b)
<span class="ot">&gt;</span>           (constF u (fmap unDual m))
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> _test_dual_pointwise_gradient_by
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Num</span> r, <span class="dt">Ord</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> <span class="dt">Metric</span> <span class="ot">-&gt;</span> <span class="dt">String</span> <span class="ot">-&gt;</span> r <span class="ot">-&gt;</span> (<span class="dt">Dual</span> r <span class="ot">-&gt;</span> <span class="dt">Dual</span> r) <span class="ot">-&gt;</span> (r <span class="ot">-&gt;</span> r)
<span class="ot">&gt;</span>   <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> _test_dual_pointwise_gradient_by metric name r p q <span class="fu">=</span>
<span class="ot">&gt;</span>   testName (<span class="st">&quot;dual pointwise gradient (&quot;</span> <span class="fu">++</span> name <span class="fu">++</span> <span class="st">&quot;)&quot;</span>) <span class="fu">$</span>
<span class="ot">&gt;</span>   \u <span class="ot">-&gt;</span> u <span class="fu">~/=</span> <span class="dv">0</span> <span class="fu">==&gt;</span> 
<span class="ot">&gt;</span>     _test_functions_equal metric (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">6</span>))
<span class="ot">&gt;</span>       (dualGrad <span class="fu">$</span> pointwiseF u p)
<span class="ot">&gt;</span>       (diagF u <span class="fu">$.</span> (pointwiseF u q))
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> _test_dual_direct_sum_left_gradient
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Num</span> r, <span class="dt">Ord</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> _test_dual_direct_sum_left_gradient r <span class="fu">=</span>
<span class="ot">&gt;</span>   testName <span class="st">&quot;direct sum on left dual gradient&quot;</span> <span class="fu">$</span>
<span class="ot">&gt;</span>   \u v <span class="ot">-&gt;</span> (u <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">&amp;&amp;</span> (v <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">==&gt;</span>
<span class="ot">&gt;</span>     forAll (arbTensorOf r v) <span class="fu">$</span>
<span class="ot">&gt;</span>       \m <span class="ot">-&gt;</span>
<span class="ot">&gt;</span>         _test_functions_equal <span class="dt">MaxAbsDiff</span> (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">5</span>))
<span class="ot">&gt;</span>           (dualGrad <span class="fu">$</span> dSumL u (fmap toDual m))
<span class="ot">&gt;</span>           (constF u <span class="fu">$</span> (zeros <span class="fu">$</span> v <span class="fu">:*</span> u) <span class="fu">~-~</span> (idMat u))
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span> _test_dual_direct_sum_right_gradient
<span class="ot">&gt;   ::</span> (<span class="dt">Eq</span> r, <span class="dt">Num</span> r, <span class="dt">Ord</span> r, <span class="dt">Fractional</span> r, <span class="dt">Show</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Test</span> (<span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Size</span> <span class="ot">-&gt;</span> <span class="dt">Property</span>)
<span class="ot">&gt;</span> _test_dual_direct_sum_right_gradient r <span class="fu">=</span>
<span class="ot">&gt;</span>   testName <span class="st">&quot;direct sum on right dual gradient&quot;</span> <span class="fu">$</span>
<span class="ot">&gt;</span>   \u v <span class="ot">-&gt;</span> (u <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">&amp;&amp;</span> (v <span class="fu">~/=</span> <span class="dv">0</span>) <span class="fu">==&gt;</span>
<span class="ot">&gt;</span>     forAll (arbTensorOf r v) <span class="fu">$</span>
<span class="ot">&gt;</span>       \m <span class="ot">-&gt;</span>
<span class="ot">&gt;</span>         _test_functions_equal <span class="dt">MaxAbsDiff</span> (<span class="dv">10</span><span class="fu">^^</span>(<span class="fu">-</span><span class="dv">5</span>))
<span class="ot">&gt;</span>           (dualGrad <span class="fu">$</span> dSumR u (fmap toDual m))
<span class="ot">&gt;</span>           (constF u <span class="fu">$</span> (idMat u) <span class="fu">~-~</span> (zeros <span class="fu">$</span> v <span class="fu">:*</span> u))</code></pre></div>
<h2 id="test-suite">Test Suite</h2>
<div class="sourceCode"><pre class="sourceCode literate literatehaskell"><code class="sourceCode literatehaskell"><span class="ot">&gt;</span> _test_gradient_checking
<span class="ot">&gt;   ::</span> (<span class="dt">Show</span> r, <span class="dt">Fractional</span> r, <span class="dt">Ord</span> r, <span class="dt">Num</span> r, <span class="dt">Floating</span> r, <span class="dt">Arbitrary</span> r)
<span class="ot">&gt;</span>   <span class="ot">=&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()
<span class="ot">&gt;</span> _test_gradient_checking r num size <span class="fu">=</span> <span class="kw">do</span>
<span class="ot">&gt;</span>   testLabel <span class="st">&quot;Gradient&quot;</span>
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   <span class="kw">let</span>
<span class="ot">&gt;</span>     args <span class="fu">=</span> stdArgs
<span class="ot">&gt;</span>       { maxSuccess <span class="fu">=</span> num
<span class="ot">&gt;</span>       , maxSize <span class="fu">=</span> size
<span class="ot">&gt;</span>       }
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   testLabel <span class="st">&quot;Dual&quot;</span>
<span class="ot">&gt;</span>   runTest args (_test_dual_scalar_gradient r)
<span class="ot">&gt;</span>   runTest args (_test_dual_linear_gradient r)
<span class="ot">&gt;</span>   runTest args (_test_dual_affine_gradient r)
<span class="ot">&gt;</span>   runTest args (_test_dual_pointwise_gradient_by <span class="dt">MaxAbsDiff</span>
<span class="ot">&gt;</span>                  <span class="st">&quot;^2&quot;</span>  r (<span class="fu">^</span><span class="dv">2</span>) (<span class="fu">*</span><span class="dv">2</span>))
<span class="ot">&gt;</span>   runTest args (_test_dual_pointwise_gradient_by <span class="dt">MaxAbsDiff</span>
<span class="ot">&gt;</span>                  <span class="st">&quot;sin&quot;</span> r sin cos)
<span class="ot">&gt;</span>   runTest args (_test_dual_pointwise_gradient_by <span class="dt">MaxAbsDiff</span>
<span class="ot">&gt;</span>                  <span class="st">&quot;cos&quot;</span> r cos (negate <span class="fu">.</span> sin))
<span class="ot">&gt;</span>   runTest args (_test_dual_pointwise_gradient_by <span class="dt">MaxAbsDiff</span>
<span class="ot">&gt;</span>                  <span class="st">&quot;exp&quot;</span> r exp exp)
<span class="ot">&gt;</span>   runTest args (_test_dual_direct_sum_left_gradient r)
<span class="ot">&gt;</span>   runTest args (_test_dual_direct_sum_right_gradient r)
<span class="ot">&gt;</span> 
<span class="ot">&gt;</span>   testLabel <span class="st">&quot;Numerical&quot;</span>
<span class="ot">&gt;</span>   runTest args (_test_numerical_scalar_gradient r)
<span class="ot">&gt;</span>   runTest args (_test_numerical_linear_gradient r)
<span class="ot">&gt;</span>   runTest args (_test_numerical_affine_gradient r)
<span class="ot">&gt;</span>   runTest args (_test_numerical_pointwise_gradient_by <span class="dt">MaxAbsDiff</span>
<span class="ot">&gt;</span>                  <span class="st">&quot;^2&quot;</span>  r (<span class="fu">^</span><span class="dv">2</span>) (<span class="fu">*</span><span class="dv">2</span>))
<span class="ot">&gt;</span>   runTest args (_test_numerical_pointwise_gradient_by <span class="dt">MaxAbsDiff</span>
<span class="ot">&gt;</span>                  <span class="st">&quot;sin&quot;</span> r sin cos)
<span class="ot">&gt;</span>   runTest args (_test_numerical_pointwise_gradient_by <span class="dt">MaxAbsDiff</span>
<span class="ot">&gt;</span>                  <span class="st">&quot;cos&quot;</span> r cos (negate <span class="fu">.</span> sin))
<span class="ot">&gt;</span>   runTest args (_test_numerical_pointwise_gradient_by <span class="dt">MaxRelDiff</span>
<span class="ot">&gt;</span>                  <span class="st">&quot;exp&quot;</span> r exp exp)
<span class="ot">&gt;</span>   runTest args (_test_numerical_direct_sum_left_gradient r)
<span class="ot">&gt;</span>   runTest args (_test_numerical_direct_sum_right_gradient r)
<span class="ot">&gt;</span> 
<span class="ot">&gt; main_gradient_checking ::</span> <span class="dt">IO</span> ()
<span class="ot">&gt;</span> main_gradient_checking <span class="fu">=</span> _test_gradient_checking (<span class="dv">0</span><span class="ot"> ::</span> <span class="dt">Double</span>) <span class="dv">100</span> <span class="dv">3</span></code></pre></div>



<!-- END BODY -->
</div>

<div id="footer">
  Site generated by
  <a href="http://jaspervdj.be/hakyll">Hakyll</a>
</div>
</body>
</html>
